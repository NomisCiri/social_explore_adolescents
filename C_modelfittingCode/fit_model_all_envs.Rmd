---
title: "Modelfitting"
author: "Simon&Andrea"
date: '2022-10-120'
output: html_document
---

```{r setup, include=FALSE}
pacman::p_load(tidyverse, rjson, DEoptim, doParallel, here)
knitr::opts_knit$set(root.dir = here()) # set root fot the whole file
```

In this document, we will fit a kalman filter to the "social bandit" task that Andrea and I developed. 
First, lets load some data.

```{r}
source("./C_modelfittingCode/models.R") # more reproducible

# load behavioral data
explore_data <- read_csv(file = paste0("./Data/data_coord.csv"))
```

```{r}
modelFit <-
  function(par,
           subjD,
           acquisition,
           k,
           horizonLength,
           rounds) {
    # Extract and process parameters

    if (inherits(acquisition, "epsilonGreedy")) {
      epsilon <-
        1 / (1 + exp(-(par[length(par)]))) # transform back from unbounded space; epsilon is the last parameter for epsilon greedy
    }

    par <- exp(par) # exponentiate parameters to make a non-negative and convex optimization surface
    # last parameter for all other models is always inverse temperature for softmax
    tau <- par[length(par)]
    # Which posterior function to use; therefore, which parameters to use

    if (inherits(k, "KalmanFilter")) {
      # null kernel indicates kalman filter model
      kNoise <- par[1]
      parVec <- c(kNoise) # Vector of parameters to send to the KF posterior function
    } else if (inherits(k, "GP")) {
      # lambda
      lambda <- par[1]
      parVec <-
        c(lambda, lambda, 1, .0001) # Vector of parameters to send to the GP posterior vector, where sF and sN are fixed
    } else if (inherits(k, "bmt_free_priors")) {
      prior_mu <- log(par[1]) # put back into native space between -100 and 100
      prior_var <- par[2]
      kNoise <- par[3]
      parVec <- c(prior_mu, prior_var, kNoise)
    }

    # Additional acquisition function dependent parameters

    if (inherits(acquisition, "UCB") |
      inherits(acquisition, "exploreCounts") |
      inherits(acquisition, "epsilonGreedy")) {
      # check if UCB is used
      beta <- par[length(par) - 1] # If UCB, beta is always 2nd last

      # refactor beta and tau into gamma and beta_star, where gamma = 1/tau and beta_star = beta/tau
    }
    # which rounds to consider?

    trainingSet <- subset(subjD, round %in% rounds)

    # Vector to store negative log likelihods

    nLL <- rep(0, length(rounds))

    for (r in unique(trainingSet$round)) {
      # Begin looping through each round
      # subset of data for round r

      roundD <- subset(subjD, round == r)
      horizon <- nrow(roundD)

      # Observations of subject choice behavior

      chosen <- roundD$cells
      chosen <-
        chosen[2:length(chosen)] # trim first observation, since it wasn't a choice but a randomly revealed tile
      y <-
        roundD$z[0:(horizon - 1)] # trim off the last observation, because it was not used to inform a choice (round already over)
      x1 <- roundD$x[0:(horizon - 1)]
      x2 <- roundD$y[0:(horizon - 1)]
      # create observation matrix
      X <- as.matrix(cbind(x1, x2))
      # make sure X is a matrix
      X <- as.matrix(X)
      Xnew <- as.matrix(Xnew)
      # Utilties of each choice
      utilities <- NULL
      prevPost <-
        NULL # set the previous posterior computation to NULL for the kalman filter
      pMat <- NULL
      # loop through observations
      for (i in 1:(horizon - 1)) {
        # skip the last observation, because no choice was made based on that information
        # new observation
        X1 <- matrix(X[1:i, ], ncol = 2)
        y1 <- y[1:i]
        # Which posterior function to use
        if (inherits(k, "KalmanFilter")) {
          # kalman filter model
          out <-
            bayesianMeanTracker(
              x = X1[i, ],
              y = y[i],
              prevPost = prevPost,
              theta = parVec
            )
          # update prevPost for the next round
          prevPost <- out
        } else if (inherits(k, "bmt_free_priors")) {
          # kalman filter model with free priors
          out <-
            bmt_free_priors(
              x = X1[i, ],
              y = y[i],
              prevPost = prevPost,
              theta = parVec
            )
          # update prevPost for the next round
          prevPost <- out
        } else if (inherits(k, "GP")) {
          # GP with length-scale parameterized kernel
          out <-
            gpr(
              X.test = Xnew,
              theta = parVec,
              X = X1,
              Y = y1,
              k = k
            ) # Mu and Sigma predictions for each of the arms; either GP or Kalman filter
        } else if (inherits(k, "Null")) {
          # null model
          out <-
            nullModel() # Mu and Sigma predictions for each of the arms; either GP or Kalman filter
        }
        # Slightly different function calls for each acquisition function
        if (inherits(acquisition, "UCB")) {
          # UCB takes a beta parameter
          utilityVec <- acquisition(out, c(beta))
        } else if (inherits(acquisition, "exploreCounts")) {
          # count-based exploration
          utilityVec <-
            exploreCounts(out, roundD$chosen[1:i], c(beta))
        } else if (inherits(acquisition, "epsilonGreedy")) {
          p <- epsilonGreedy(out, beta, epsilon)
          pMat <- rbind(pMat, t(p))
        } else {
          # any other
          utilityVec <- acquisition(out)
        }
        if (inherits(acquisition, "softmax")) {
          utilityVec <- utilityVec - max(utilityVec) # avoid overflow
          utilities <-
            rbind(utilities, t(utilityVec)) # build horizon_length x options matrix, where each row holds the utilities of each choice at each decision time in the search horizon
        }
      }
      # print(utilities)
      if (inherits(acquisition, "softmax")) {
        # Softmax rule
        p <- exp(utilities / tau)
        p <- p / rowSums(p)
        # avoid underflow by setting a floor and a ceiling
        p <- (pmax(p, 0.00001))
        p <- (pmin(p, 0.99999))
        pMat <- p
      }
      # Calculate Negative log likelihood
      nLL[which(unique(trainingSet$round) == r)] <-
        -sum(log(pMat[cbind(c(1:(horizon - 1)), chosen)]))
    }
    # end loop through rounds
    return(sum(nLL)) # Return negative log likelihoods of all observations
  }
```


# Define function for fitting

```{r}
cvfun <- function(data,
                  kernelFun,
                  acquisition,
                  leaveoutindex) {
  # subselect participant, horizon and rounds not left out

  # andrea changed: only round with gems
  d1 <- data
  # training set
  # get round names
  rounds <- unique(d1$round)
  # browser()
  # rounds <- 1:6
  # rounds <- count(d1$round)[count(d1$round)$freq>=2,]$x #only rounds where at least 2 clicks have been made
  trainingSet <-
    rounds[!rounds == leaveoutindex] # remove round specified by leaveoutindex
  # test set
  testSet <- leaveoutindex
  nParams <- 1
  if (inherits(acquisition, "UCB") |
    inherits(acquisition, "exploreCounts") |
    inherits(acquisition, "epsilonGreedy")) {
    nParams <- nParams + 1 # add beta parameter
  }
  if (inherits(kernelFun, "GP") |
    inherits(kernelFun, "KalmanFilter")) {
    nParams <- nParams + 1 # add lambda or error variance
    lbound <- c(-10, -10)
    ubound <- c(6.5, 0)
  } else if (inherits(kernelFun, "bmt_free_priors")) {
    nParams <- nParams + 3 # add error variance and priors
  }
  # Set upper and lower bounds based on nParams this is logspace!
  # lbound <- rep(-4, nParams)
  # ubound <- rep(5, nParams)
  # new bounds=
  # lbound <- rep(-10, nParams)
  # ubound <- rep(2, nParams)

  if (inherits(kernelFun, "bmt_free_priors")) {
    # no logspace on the boundaries of the prior mu.
    lbound <- c(-100, -10, -10, -10)
    ubound <- c(100, 8, 8, 0)
  }

  if (nParams >= 2) {
    # if 2 or more parameters
    # TRAINING SET

    # this here.
    fit <-
      DEoptim(
        modelFit,
        lower = lbound,
        upper = ubound,
        subjD = d1,
        k = kernelFun,
        rounds = trainingSet,
        acquisition = acquisition,
        DEoptim.control(itermax = 100)
      )

    paramEstimates <-
      fit$optim$bestmem # MODEL DEPENDENT PARAMETER ESTIMATES
    # TEST SET
    predict <-
      modelFit(
        par = paramEstimates,
        subjD = d1,
        acquisition = acquisition,
        k = kernelFun,
        rounds = testSet
      )
    output <-
      c(leaveoutindex, predict, fit$optim$bestmem) # leaveoutindex, nLL, parameters....
  } else {
    # TRAINING SET
    fit <-
      optimize(
        modelFit,
        lower = lbound,
        upper = ubound,
        subjD = d1,
        k = kernelFun,
        rounds = trainingSet,
        acquisition = acquisition
      )

    paramEstimates <-
      fit$minimum # MODEL DEPENDENT PARAMETER ESTIMATES
    # TEST SET
    predict <-
      modelFit(
        par = paramEstimates,
        subjD = d1,
        acquisition = acquisition,
        k = kernelFun,
        rounds = testSet
      )

    output <-
      c(leaveoutindex, predict, fit$minimum) # leaveoutindex, nLL, parameters....
  }

  return(output) # return optimized value
}
```

# Fit

```{r}
# dummy environment

# parallelize over players
doParallel::registerDoParallel()
n.cores <- parallel::detectCores() - 1

# create the cluster
my.cluster <- parallel::makeCluster(
  n.cores,
  type = "PSOCK"
)
doParallel::registerDoParallel(cl = my.cluster)
```

# fit kalman filter

```{r eval=F}
foreach(
  playerNr = unique(explore_data$player),
  .packages = c("DEoptim", "dplyr")
) %dopar% {
  # browser()
  Xnew <- as.matrix(expand.grid(0:7, 0:7)) # do this outside the loop for better speed
  output <- c()

  # preallocate round selection because we subset by gems and we cant simply count from 1 to n anymore...
  # andrea: maybe we can now do this inside the function since we don't split anymore?

  d1 <- subset(explore_data, player == playerNr) %>%
    mutate(z = (points - mean(points)) / sd(points))

  print(d1)

  rounds <- unique(d1$round)

  for (r in rounds) { # loop through rounds in roundList
    cv <- cvfun(data = d1, kernelFun = bayesianMeanTracker, acquisition = greedyMean, leaveoutindex = r) # only try one sub
    output <- rbind(output, cv)
  }

  saveRDS(output, file = paste0("A_GeneratedFiles/modelfits/all_envs/GM_fit_", playerNr, ".rds"))
}
```







# fit kalmanfilter with free priors

```{r}
getwd()

foreach(
  playerNr = unique(explore_data$player),
  .packages = c("DEoptim", "dplyr")
) %dopar% {
  # browser()
  Xnew <- as.matrix(expand.grid(0:7, 0:7)) # do this outside the loop for better speed
  output <- c()

  # preallocate round selection because we subset by gems and we cant simply count from 1 to n anymore...
  # andrea: maybe we can now do this inside the function since we don't split anymore?

  d1 <- subset(explore_data, player == playerNr) %>%
    mutate(z = (points - mean(points)) / sd(points))

  print(d1)

  rounds <- unique(d1$round)

  for (r in rounds) { # loop through rounds in roundList
    cv <- cvfun(data = d1, kernelFun = bmt_free_priors, acquisition = greedyMean, leaveoutindex = r) # only try one sub
    output <- rbind(output, cv)
  }
  saveRDS(output, file = paste0("A_GeneratedFiles/modelfits/all_envs/GM_free_prior_fit_", playerNr, ".rds"))
}
```










# fit that thing in parallel

```{r eval=F}
foreach(
  playerNr = unique(explore_data$player),
  .packages = c("DEoptim", "dplyr")
) %dopar% {
  # browser()
  Xnew <- as.matrix(expand.grid(0:7, 0:7)) # do this outside the loop for better speed
  output <- c()

  # preallocate round selection because we subset by gems and we cant simply count from 1 to n anymore...
  # andrea: maybe we can now do this inside the function since we don't split anymore?

  d1_round <- subset(explore_data, player == playerNr) %>%
    group_by(round) %>%
    mutate(z = (points - mean(points)) / sd(points)) %>%
    ungroup()

  # print(d1_round)

  rounds <- unique(d1_round$round)

  for (r in rounds) { # loop through rounds in roundList
    cv <- cvfun(data = d1_round, kernelFun = bayesianMeanTracker, acquisition = greedyMean, leaveoutindex = r) # only try one sub
    output <- rbind(output, cv)
  }

  saveRDS(output, file = paste0("A_GeneratedFiles/modelfits/all_envs/GM_fit_New_Bounds_by_round", playerNr, ".rds"))
}
```







# fit kalmanfilter with free priors

```{r}
getwd()

foreach(
  playerNr = unique(explore_data$player),
  .packages = c("DEoptim", "dplyr")
) %dopar% {
  # browser()
  Xnew <- as.matrix(expand.grid(0:7, 0:7)) # do this outside the loop for better speed
  output <- c()

  # preallocate round selection because we subset by gems and we cant simply count from 1 to n anymore...
  # andrea: maybe we can now do this inside the function since we don't split anymore?

  d1 <- subset(explore_data, player == playerNr) %>%
    mutate(z = (points - mean(points)) / sd(points))

  print(d1)

  rounds <- unique(d1$round)

  for (r in rounds) { # loop through rounds in roundList
    cv <- cvfun(data = d1, kernelFun = bmt_free_priors, acquisition = greedyMean, leaveoutindex = r) # only try one sub
    output <- rbind(output, cv)
  }
  saveRDS(output, file = paste0("A_GeneratedFiles/modelfits/all_envs/GM_free_prior_fit_", playerNr, ".rds"))
}
```
