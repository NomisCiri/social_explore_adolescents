---
title: "simulate_from_experimental_data"
author: "Andrea"
date: "2022-08-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# load packages
pacman::p_load(tidyverse, rjson, DEoptim, doParallel, jsonlite, data.table,here)
knitr::opts_knit$set(root.dir = here())# set root fot the whole file

# functions to do the model fitting
```

Simulate choice behavior from the parameters fitted to the pilot of the experiment run in schools.

```{r}
source("C_modelfittingCode/models.R")

# load the experimental data
explore_data <- read_csv(file = "Data/data_coord.csv")

# load the generated environments
envs_no_gems_list <-
  rjson::fromJSON(file = "A_GeneratedFiles/environments_no_gem_var25max.json") # max gem value: 250; variance: 25

envs_gems_list <-
  rjson::fromJSON(file = "A_GeneratedFiles/environments_gem_250_var25max.json") # max gem value: 250; variance: 25

# merge environments together and add identifier

envs_no_gems <- map(envs_no_gems_list, as.data.table)
envs_no_gems <- rbindlist(envs_no_gems) %>% 
  mutate(gems = 1)

envs_gems <- map(envs_gems_list, as.data.table)
envs_gems <- rbindlist(envs_gems) %>% 
  mutate(gems = 0)

envs <- rbind(envs_gems, envs_no_gems)


```



```{r}

# Function to simulate data from parameters estimated from model fitting procedure.
# Takes as input the parameters, the data, the

modelSim <-
  function(par,
           subjD,
           acquisition,
           k,
           horizonLength,
           rounds,
           env_numbers) {

    #  Extract and process parameters
    if (inherits(acquisition, "epsilonGreedy")) {
      epsilon <-
        1 / (1 + exp(-(par[length(par)]))) #transform back from unbounded space; epsilon is the last parameter for epsilon greedy
    }
    
    # Exponentiate parameters to make a non-negative and convex optimization surface
    par <- exp(par)
    
    # Last parameter for all other models is always inverse temperature for softmax
    tau <- par[length(par)]
    
    #Which posterior function to use; therefore, which parameters to use
    if (inherits(k, "KalmanFilter")) {
      #null kernel indicates kalman filter model
      kNoise <- par[1]
      parVec <-
        c(kNoise) #Vector of parameters to send to the KF posterior function
    } else if (inherits(k, "GP")) {
      #lambda
      lambda <- par[1]
      parVec <-
        c(lambda, lambda, 1, .0001) # Vector of parameters to send to the GP posterior vector, where sF and sN are fixed
    } else if (inherits(k,'bmt_free_priors')){
      prior_mu<-log(par[1])# put back into native space between -100 and 100
      prior_var<-par[2]
      kNoise<-par[3]
      parVec<-c(prior_mu,prior_var,kNoise)
  }
    
    
    #Additional acquisition function dependent parameters
    if (inherits(acquisition, "UCB") |
        inherits(acquisition, 'exploreCounts') |
        inherits(acquisition, 'epsilonGreedy')) {
      
      #check if UCB is used
      beta <- par[length(par) - 1] #If UCB, beta is always 2nd last
      #refactor beta and tau into gamma and beta_star, where gamma = 1/tau and beta_star = beta/tau
    }
    
    #which rounds to consider? @simon: is this working? now it takes everything
    trainingSet <- subset(subjD, round %in% rounds)
    
    #Vector to store negative log likelihoods
    nLL <- rep(0, length(rounds))
    
    # simple counter to know which iteration this is
    counter <- 0
    choices_round <- data.frame(choice_index = numeric(0),
                             points_new_choice = numeric(0),
                             x_new_choice = numeric(0),
                             y_new_choice = numeric(0),
                             z_new_choice = numeric(0),
                             trial = numeric(0),
                             env_number = numeric(0), 
                             env_counter = numeric(0),
                             env_type = numeric(0)
                             )
    
    for (r in unique(trainingSet$round)) {
      
      #Begin looping through each round
      #subset of data for round r
      
      # @andreaL don't need the counter anymore, now it's a sequence
      counter <- counter + 1
      
      roundD <- subset(subjD, round == r)
      horizon <- nrow(roundD)
      
      # Which tiles where clicked?
      chosen <- roundD$cells
      
      # trim first observation, since it wasn't a choice but a randomly revealed tile
      chosen <- chosen[2:length(chosen)] 
      
      ### Get first observation, to start the cycle of simulating choices
      
      points <- roundD$points[1] #absolute n of points
      y  <- roundD$z[1] # rewards (standardized at environment type level, maybe refine?)
      x1 <- roundD$x[1] # x position
      x2 <- roundD$y[1] # y position
      is_gem <- roundD$gempresent[1]
      
      # bind into an observation matrix
      X <- as.matrix(cbind(x1, x2))
      
      # @simon: do we need this line?
      # Xnew <- as.matrix(Xnew)
      
      # Utilties of each choice
      
      utilities <- NULL
      prevPost <-  NULL #set the previous posterior computation to NULL for the kalman filter 
      pMat <- NULL
      choices <- NULL
      
      # loop through observations I (until i - 1)
      # skip the last observation, because no choice was made based on that information

      
      for (i in 1:(horizon - 1)) {
        
        #browser()
        # in first round, get the real choice of the participant and associated reward
        if (i == 1){

        X1 <- matrix(X[1:i, ], ncol = 2) 
        y1 <- y[1:i]
        
        #make df to update over loop
        choices <- data.frame(choice_index = x1 + 1 + (x2 * 8),# interesting!
                             points_new_choice = points,
                             x_new_choice = x1,
                             y_new_choice = x2,
                             z_new_choice = y,
                             trial = 1, 
                             env_number = r, 
                             env_counter = counter, 
                             env_type = is_gem)
        
        } else { 
          
        X1 <- matrix(choices[1:i, 3:4], ncol = 2) 
        y <- choices[1:i, 5]
          
                  }
        
        # Which posterior function to use
        
        if (inherits(k, "KalmanFilter")) {
        
          # kalman filter model
          out <-
            bayesianMeanTracker(
              x = X1[i, ],
              y = y[i],
              prevPost = prevPost,
              theta = parVec
            )
          
        #update prevPost for the next round
           prevPost <- out
         } else if (inherits(k, "bmt_free_priors")) {
          # kalman filter model with free priors
          out <-
            bmt_free_priors(
              x = X1[i, ],
              y = y[i],
              prevPost = prevPost,
              theta = parVec
            )
          #update prevPost for the next round
          prevPost <- out
         }
        #   # GP with length-scale parameterized kernel
        #   out <-
        #     gpr(
        #       X.test = Xnew,
        #       theta = parVec,
        #       X = X1,
        #       Y = y1,
        #       k = k
        #     ) #Mu and Sigma predictions for each of the arms; either GP or Kalman filter
        # } else if (inherits(k, 'Null')) {
        #   #null model
        #   out <-
        #     nullModel() #Mu and Sigma predictions for each of the arms; either GP or Kalman filter
        # }
          
        #Slightly different function calls for each acquisition function
        if (inherits(acquisition, "UCB")) {
          #UCB takes a beta parameter
          utilityVec <- acquisition(out, c(beta))
          
        } else if (inherits(acquisition, 'exploreCounts')) {
          #count-based exploration
          utilityVec <-
            exploreCounts(out, roundD$chosen[1:i], c(beta))
          
        } else if (inherits(acquisition, "epsilonGreedy")) {
          p <- epsilonGreedy(out, beta, epsilon)
          pMat <- rbind(pMat, t(p))
          
        } else{
          #any other
          utilityVec <- acquisition(out)
        }
          
        if (inherits(acquisition, "softmax")) {
          utilityVec <- utilityVec - max(utilityVec) #avoid overflow
          
          utilities <-
            rbind(utilities, t(utilityVec)) # build horizon_length x options matrix, where each row holds the utilities of each choice at each decision time in the search horizon
        }
          
        # use softmax to transform utilites inti probabilities
        
        p <- exp(utilities / tau)
        p <- p / rowSums(p)
        
        #avoid underflow by setting a floor and a ceiling
        p <- (pmax(p, 0.00001))
        p <- (pmin(p, 0.99999))
        
        pMat <- p
        
        # make a choice based on the probabilites
        # are there gems or not?
        
        # WRITE LITTLE FUNCTION HERE
          
        this_env <- env_numbers[counter,]
        
        if (this_env$gempresent == 1){
          
          this_env_data <- envs_gems_list[this_env$env_number]
          env_type = 1
          
        } else {
          
         this_env_data <- envs_no_gems_list[this_env$env_number]
         env_type = 0

        }
        
        #browser()
        new_choice <- make_a_choice(pMat, this_env_data, i, subjD, counter, is_gem)
        choices <- as.matrix(rbind(choices, new_choice))
        #prevPost <- out
          
      }
      #browser()
      choices_round <- rbind(choices_round, choices)
  
    }
    #  end of loop through rounds
    
    # check if free priors or kalman filter and add respective parameters.
    if (inherits(k, "KalmanFilter")){
    choices_round <- choices_round %>% 
      mutate(playerNr = subjD$player[1],
             prior_mu = NA,
             prior_var=NA,
             learning_rate = parVec,
             temperature = tau)
    }else if (inherits(k, "bmt_free_priors")) {
          # kalman filter model with free priors
      choices_round <- choices_round %>% 
      mutate(playerNr = subjD$player[1],
             prior_mu = parVec[1],
             prior_var=parVec[2],
             learning_rate=parVec[3],
             temperature = tau)
    }
    
    return(choices_round)  
  }


```


```{r}
# not necessary to parallelize computations

foreach(
  
  playerNr = unique(explore_data$player),
  .packages = c("DEoptim", "dplyr")
) %do% {
  
  # load parameter vector and filter based only on succesfull rounds
  
  model_fit <-
    as.data.frame(readRDS(paste0("A_GeneratedFiles/modelfits/all_envs/GM_fit_New_Bounds_", playerNr, ".rds"))) %>% 
    filter(V2 < -log(1/64)*25)
  
  # get out of sample fit index
  LL<-mean(model_fit$V2)
  n_worse_than_chance<-12-length(model_fit$V2)
  
    # calculate the mean of the parameters fitted (currently 2 with GM algorithm),
  # and put them in one vector
  par <- c(mean(model_fit[, 3]), mean(model_fit[, 4])) 
  
  d1 <- subset(explore_data, player == playerNr) %>% 
    mutate(mean_points = mean(points),
           sd_points = sd(points) ,
           z=(points-mean_points)/sd_points)

  rounds <- unique(d1$round)
  
  env_numbers <- d1 %>% 
    select(env_number, round, gempresent) %>% 
    distinct() %>% 
    select(env_number, gempresent)
  
  Xnew <-
    as.matrix(expand.grid(0:7, 0:7)) #do this outside the loop for better speed
  
  #PMat_allrounds = array(NA, dim=c(24,65,6))
  # create df of choices by round
  
  
  # @ANDREA delete if not useful when finished
  # choices <- data.frame(round = numeric(0),
  #            choice = numeric(0),
  #            env_number = numeric(0),
  #            rewards = numeric(0))

  simulated_choices <- NULL
  
  ### only when debugging:

  # subjD = d1
  # acquisition = greedyMean
  # k = bayesianMeanTracker

  ###
  
  simulated_choices <-
    modelSim(
      par = par,
      subjD = d1,
      acquisition = greedyMean,
      k = bayesianMeanTracker,
      rounds = rounds,
      env_numbers = env_numbers
    )
  
  simulated_choices$LL=LL
  simulated_choices$n_worse_than_chance=n_worse_than_chance
  # print to know where we are
  print(paste("player", playerNr, "done"))
  
  saveRDS(simulated_choices,
          file = paste0("A_GeneratedFiles/simulations/all_envs/simulated_choices_old_bounds", playerNr))

}

```

# simulate oringinal fit
```{r}
# not necessary to parallelize computations

foreach(
  
  playerNr = unique(explore_data$player),
  .packages = c("DEoptim", "dplyr")
) %do% {
  
  # load parameter vector and filter based only on succesfull rounds
  
  model_fit <-
    as.data.frame(readRDS(paste0("A_GeneratedFiles/modelfits/all_envs/GM_fit_New_Bounds_", playerNr, ".rds"))) %>% 
    filter(V2 < -log(1/64)*25)
  
  # get out of sample fit index
  LL<-mean(model_fit$V2)
  n_worse_than_chance<-12-length(model_fit$V2)
  
    # calculate the mean of the parameters fitted (currently 2 with GM algorithm),
  # and put them in one vector
  par <- c(mean(model_fit[, 3]), mean(model_fit[, 4])) 
  
  d1 <- subset(explore_data, player == playerNr) %>% 
    mutate(mean_points = mean(points),
           sd_points = sd(points) ,
           z=(points-mean_points)/sd_points)

  rounds <- unique(d1$round)
  
  env_numbers <- d1 %>% 
    select(env_number, round, gempresent) %>% 
    distinct() %>% 
    select(env_number, gempresent)
  
  Xnew <-
    as.matrix(expand.grid(0:7, 0:7)) #do this outside the loop for better speed
  
  #PMat_allrounds = array(NA, dim=c(24,65,6))
  # create df of choices by round
  
  
  # @ANDREA delete if not useful when finished
  # choices <- data.frame(round = numeric(0),
  #            choice = numeric(0),
  #            env_number = numeric(0),
  #            rewards = numeric(0))

  simulated_choices <- NULL
  
  ### only when debugging:

  # subjD = d1
  # acquisition = greedyMean
  # k = bayesianMeanTracker

  ###
  
  simulated_choices <-
    modelSim(
      par = par,
      subjD = d1,
      acquisition = greedyMean,
      k = bayesianMeanTracker,
      rounds = rounds,
      env_numbers = env_numbers
    )
  
  simulated_choices$LL=LL
  simulated_choices$n_worse_than_chance=n_worse_than_chance
  # print to know where we are
  print(paste("player", playerNr, "done"))
  
  saveRDS(simulated_choices,
          file = paste0("A_GeneratedFiles/simulations/all_envs/simulated_choices_new_bounds", playerNr))

}

```




# simulate free prior data

```{r}
# no need to parallelize computations

foreach(
  
  playerNr = unique(explore_data$player),
  .packages = c("DEoptim", "dplyr")
) %do% {
  
  # load parameter vector and filter based only on succesfull rounds
  
  model_fit <-
    as.data.frame(readRDS(paste0("A_GeneratedFiles/modelfits/all_envs/GM_free_prior_fit_", playerNr, ".rds"))) %>% 
    filter(V2 < -log(1/64)*25)
  
   # get out of sample fit index
  LL<-mean(model_fit$V2)
  n_worse_than_chance<-12-length(model_fit$V2)
  
  # calculate the mean of the parameters fitted (currently 2 with GM algorithm),
  # and put them in one vector
  
  par <- c(mean(model_fit[, 3]), mean(model_fit[, 4]),mean(model_fit[, 5]), mean(model_fit[, 6]))
  
  d1 <- subset(explore_data, player == playerNr) %>% 
    mutate(mean_points = mean(points),
           sd_points = sd(points) ,
           z=(points-mean_points)/sd_points)

  rounds <- unique(d1$round)
  
  env_numbers <- d1 %>% 
    select(env_number, round, gempresent) %>% 
    distinct() %>% 
    select(env_number, gempresent)
  
  Xnew <-
    as.matrix(expand.grid(0:7, 0:7)) #do this outside the loop for better speed
  
  #PMat_allrounds = array(NA, dim=c(24,65,6))
  # create df of choices by round
  
  
  # @ANDREA delete if not useful when finished
  # choices <- data.frame(round = numeric(0),
  #            choice = numeric(0),
  #            env_number = numeric(0),
  #            rewards = numeric(0))

  simulated_choices <- NULL
  
  ### only when debugging:

  # subjD = d1
  # acquisition = greedyMean
  # k = bayesianMeanTracker

  ###
  
  simulated_choices <-
    modelSim(
      par = par,
      subjD = d1,
      acquisition = greedyMean,
      k = bmt_free_priors,
      rounds = rounds,
      env_numbers = env_numbers
    )
  
  simulated_choices$LL=LL
  simulated_choices$n_worse_than_chance=n_worse_than_chance
  
  # print to know where we are
  print(paste("player", playerNr, "done"))
  
  saveRDS(simulated_choices,
          file = paste0("A_GeneratedFiles/simulations/all_envs/simulated_choices_free_prior", playerNr))

}

```
# simulate new bounds & within round normalization

```{r}
# not necessary to parallelize computations

foreach(
  
  playerNr = unique(explore_data$player),
  .packages = c("DEoptim", "dplyr")
) %do% {
  
  # load parameter vector and filter based only on succesfull rounds
  
  model_fit <-
    as.data.frame(readRDS(paste0("A_GeneratedFiles/modelfits/all_envs/GM_fit_New_Bounds_by_round", playerNr, ".rds"))) %>% 
    filter(V2 < -log(1/64)*25)
  
  # get out of sample fit index
  LL<-mean(model_fit$V2)
  n_worse_than_chance<-12-length(model_fit$V2)
  
    # calculate the mean of the parameters fitted (currently 2 with GM algorithm),
  # and put them in one vector
  par <- c(mean(model_fit[, 3]), mean(model_fit[, 4])) 
  
  d1 <- subset(explore_data, player == playerNr) %>% group_by(round) %>%
    mutate(mean_points = mean(points),
           sd_points = sd(points) ,
           z=(points-mean_points)/sd_points)%>%ungroup()

  rounds <- unique(d1$round)
  
  env_numbers <- d1 %>% 
    select(env_number, round, gempresent) %>% 
    distinct() %>% 
    select(env_number, gempresent)
  
  Xnew <-
    as.matrix(expand.grid(0:7, 0:7)) #do this outside the loop for better speed
  
  #PMat_allrounds = array(NA, dim=c(24,65,6))
  # create df of choices by round
  
  
  # @ANDREA delete if not useful when finished
  # choices <- data.frame(round = numeric(0),
  #            choice = numeric(0),
  #            env_number = numeric(0),
  #            rewards = numeric(0))

  simulated_choices <- NULL
  
  ### only when debugging:

  # subjD = d1
  # acquisition = greedyMean
  # k = bayesianMeanTracker

  ###
  
  simulated_choices <-
    modelSim(
      par = par,
      subjD = d1,
      acquisition = greedyMean,
      k = bayesianMeanTracker,
      rounds = rounds,
      env_numbers = env_numbers
    )
  
  simulated_choices$LL=LL
  simulated_choices$n_worse_than_chance=n_worse_than_chance
  # print to know where we are
  print(paste("player", playerNr, "done"))
  
  saveRDS(simulated_choices,
          file = paste0("A_GeneratedFiles/simulations/all_envs/simulated_choices_new_bounds_by_round", playerNr))

}

```
