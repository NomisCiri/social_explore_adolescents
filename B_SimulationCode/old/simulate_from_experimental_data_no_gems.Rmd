---
title: "simulate_from_experimental_data"
author: "Andrea"
date: "2022-08-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# load packages
pacman::p_load(tidyverse, rjson, DEoptim, doParallel)

# functions to do the model fitting
source("C_modelfittingCode/models.R")
```

Simulate choice behavior from the parameters fitted to the pilot of the experiment run in schools.

```{r}
# load the experimental data
explore_data <- read_csv(file = "Data/data_coord.csv")

# load the generated environments
envs <- fromJSON(file = "A_GeneratedFiles/environments_no_gem_var25max.json") # max gem value: 250; variance: 25
```


```{r}
# Function to simulate data from parameters estimated from model fitting procedure.
# Takes as input the parameters, the data, the

modelSim <-
  function(par,
           subjD,
           acquisition,
           k,
           horizonLength,
           rounds,
           env_numbers) {
    #  Extract and process parameters
    if (inherits(acquisition, "epsilonGreedy")) {
      epsilon <-
        1 / (1 + exp(-(par[length(par)]))) # transform back from unbounded space; epsilon is the last parameter for epsilon greedy
    }

    # Exponentiate parameters to make a non-negative and convex optimization surface
    par <- exp(par)

    # Last parameter for all other models is always inverse temperature for softmax
    tau <- par[length(par)]

    # Which posterior function to use; therefore, which parameters to use
    if (inherits(k, "KalmanFilter")) {
      # null kernel indicates kalman filter model
      kNoise <- par[1]
      parVec <-
        c(kNoise) # Vector of parameters to send to the KF posterior function
    } else if (inherits(k, "GP")) {
      # lambda
      lambda <- par[1]
      parVec <-
        c(lambda, lambda, 1, .0001) # Vector of parameters to send to the GP posterior vector, where sF and sN are fixed
    }


    # Additional acquisition function dependent parameters
    if (inherits(acquisition, "UCB") |
      inherits(acquisition, "exploreCounts") |
      inherits(acquisition, "epsilonGreedy")) {
      # check if UCB is used
      beta <- par[length(par) - 1] # If UCB, beta is always 2nd last
      # refactor beta and tau into gamma and beta_star, where gamma = 1/tau and beta_star = beta/tau
    }

    # which rounds to consider? @simon: is this working? now it takes everything
    trainingSet <- subset(subjD, round %in% rounds)

    # Vector to store negative log likelihoods
    nLL <- rep(0, length(rounds))

    # simple counter to know which iteration this is
    counter <- 0
    choices_round <- data.frame(
      choice_index = numeric(0),
      points_new_choice = numeric(0),
      x_new_choice = numeric(0),
      y_new_choice = numeric(0),
      z_new_choice = numeric(0),
      trial = numeric(0),
      env_number = numeric(0),
      env_counter = numeric(0)
    )

    for (r in unique(trainingSet$round)) {
      # Begin looping through each round
      # subset of data for round r

      counter <- counter + 1
      roundD <- subset(subjD, round == r)
      horizon <- nrow(roundD)

      # Which tiles where clicked?
      chosen <- roundD$cells

      # trim first observation, since it wasn't a choice but a randomly revealed tile
      chosen <- chosen[2:length(chosen)]

      ### Get first observation, to start the cycle of simulating choices

      points <- roundD$points[1] # absolute n of points
      y <- roundD$z[1] # rewards (standardized at environment type level, maybe refine?)
      x1 <- roundD$x[1] # x position
      x2 <- roundD$y[1] # y position

      # bind into an observation matrix
      X <- as.matrix(cbind(x1, x2))

      # @simon: do we need this line?
      # Xnew <- as.matrix(Xnew)

      # Utilties of each choice

      utilities <- NULL
      prevPost <- NULL # set the previous posterior computation to NULL for the kalman filter
      pMat <- NULL
      choices <- NULL

      # loop through observations I (until i - 1)
      # skip the last observation, because no choice was made based on that information


      for (i in 1:(horizon - 1)) {
        # browser()
        # in first round, get the real choice of the participant and associated reward
        if (i == 1) {
          X1 <- matrix(X[1:i, ], ncol = 2)
          y1 <- y[1:i]

          # make df to update over loop
          choices <- data.frame(
            choice_index = x1 + 1 + (x2 * 8),
            points_new_choice = points,
            x_new_choice = x1,
            y_new_choice = x2,
            z_new_choice = y,
            trial = 1,
            env_number = r,
            env_counter = counter
          )
        } else {
          X1 <- matrix(choices[1:i, 3:4], ncol = 2)
          y <- choices[1:i, 5]
        }

        # Which posterior function to use

        # if (inherits(k, "KalmanFilter")) {

        # kalman filter model
        out <-
          bayesianMeanTracker(
            x = X1[i, ],
            y = y[i],
            prevPost = prevPost,
            theta = parVec
          )

        # @ temporarely get rid of this @simon maybe forever? or may come back?
        #   #update prevPost for the next round
        #   prevPost <- out
        # } else if (inherits(k, 'GP')) {
        #   # GP with length-scale parameterized kernel
        #   out <-
        #     gpr(
        #       X.test = Xnew,
        #       theta = parVec,
        #       X = X1,
        #       Y = y1,
        #       k = k
        #     ) #Mu and Sigma predictions for each of the arms; either GP or Kalman filter
        # } else if (inherits(k, 'Null')) {
        #   #null model
        #   out <-
        #     nullModel() #Mu and Sigma predictions for each of the arms; either GP or Kalman filter
        # }

        # Slightly different function calls for each acquisition function
        if (inherits(acquisition, "UCB")) {
          # UCB takes a beta parameter
          utilityVec <- acquisition(out, c(beta))
        } else if (inherits(acquisition, "exploreCounts")) {
          # count-based exploration
          utilityVec <-
            exploreCounts(out, roundD$chosen[1:i], c(beta))
        } else if (inherits(acquisition, "epsilonGreedy")) {
          p <- epsilonGreedy(out, beta, epsilon)
          pMat <- rbind(pMat, t(p))
        } else {
          # any other
          utilityVec <- acquisition(out)
        }

        if (inherits(acquisition, "softmax")) {
          utilityVec <- utilityVec - max(utilityVec) # avoid overflow

          utilities <-
            rbind(utilities, t(utilityVec)) # build horizon_length x options matrix, where each row holds the utilities of each choice at each decision time in the search horizon
        }

        # use softmax to transform utilites inti probabilities

        p <- exp(utilities / tau)
        p <- p / rowSums(p)

        # avoid underflow by setting a floor and a ceiling
        p <- (pmax(p, 0.00001))
        p <- (pmin(p, 0.99999))

        pMat <- p

        # make a choice based on the probabilites
        # @Simon: would you sample more than once? otherwise pretty random

        this_env <- envs[env_numbers[counter]]

        # browser()
        new_choice <- make_a_choice(pMat, this_env, i, subjD, counter)
        choices <- as.matrix(rbind(choices, new_choice))
        prevPost <- out
      }
      # browser()
      choices_round <- rbind(choices_round, choices)
    }
    #  end of loop through rounds

    choices_round <- choices_round %>%
      mutate(
        playerNr = subjD$player[1],
        learning_rate = parVec,
        temperature = tau
      )

    return(choices_round)
  }
```
   

```{r}
# Run simulations

# parallelize over players

doParallel::registerDoParallel()
n.cores <- parallel::detectCores() - 1
# create the cluster
my.cluster <- parallel::makeCluster(
  n.cores,
  type = "PSOCK"
)
doParallel::registerDoParallel(cl = my.cluster)
```


```{r}
# parallelize computations

foreach(
  playerNr = unique(explore_data$player),
  .packages = c("DEoptim", "dplyr")
) %dopar% {
  # load parameter vector
  model_fit <-
    readRDS(paste0("A_GeneratedFiles/modelfits/GM_fit_", playerNr, ".rds"))

  # calculate the mean of the parameters fitted (currently 2 with GM algorithm),
  # and put them in one vector

  par <- c(mean(model_fit[, 3]), mean(model_fit[, 4]))

  d1 <- subset(explore_data, player == playerNr & gempresent == 0) %>%
    mutate(
      mean_points = mean(points),
      sd_points = sd(points),
      z = (points - mean_points) / sd_points
    )

  rounds <- unique(d1$round)

  env_numbers <- d1 %>%
    select(env_number, round) %>%
    distinct() %>%
    select(env_number) %>%
    pull()

  #**** to change: whole dataset, subset within loop
  Xnew <-
    as.matrix(expand.grid(0:7, 0:7)) # do this outside the loop for better speed

  # PMat_allrounds = array(NA, dim=c(24,65,6))
  # create df of choices by round


  # @ANDREA delete if not useful when finished
  # choices <- data.frame(round = numeric(0),
  #            choice = numeric(0),
  #            env_number = numeric(0),
  #            rewards = numeric(0))

  simulated_choices <- NULL

  ### only when debugging:

  subjD <- d1
  acquisition <- greedyMean
  k <- bayesianMeanTracker

  ###

  simulated_choices <-
    modelSim(
      par = par,
      subjD = d1,
      acquisition = greedyMean,
      k = bayesianMeanTracker,
      rounds = rounds,
      env_numbers = env_numbers
    )

  # print to know where we are
  print(paste("player", playerNr, "done"))

  saveRDS(simulated_choices,
    file = paste0("A_GeneratedFiles/simulations/simulated_choices", playerNr)
  )
}
```
